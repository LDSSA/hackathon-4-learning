{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topics\n",
    "\n",
    "* Collaborative filtering is a neighbohood recommendation method (done)\n",
    "* Two classical problems that we can solve:\n",
    "    * Best item problem -> metrics (done)\n",
    "    * Top N items problem -> metrics (done)\n",
    "* Explain collaborative filtering idea (done)\n",
    "* Difference between user based and item based cf (done)\n",
    "* Similarity measures: cosine, correlation (done)\n",
    "    * improve similarities by accounting for significance (more pairs of common users mean a better similarity)\n",
    "* Item based regression (for ratings) (done)\n",
    "* Item based top-n recommendations (lamire -> because this is kind of the hackathon)\n",
    "* Sparse matrixes -> you'll need these for the hackathon\n",
    "    * Example on giving movie recommendations for the last movie seen by each user\n",
    "    \n",
    "    \n",
    "(To add after checking Ascens√£o's notebook)\n",
    "* What is a recommender system\n",
    "* Explicit ratings vs implicit feedback\n",
    "* Importance of evaluating the system and explain that the evaluation metric must be carefully selected and depends a lot on the task we're trying to accomplish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "MOVIES_FILE = '../data/movies.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering algorithms for recommendations\n",
    "\n",
    "\n",
    "Collaborative filtering is a recommendation method based on nearest-neighbors. It is commonly used in recommender systems in practice because of its simplicity and efficiency in producing good results.\n",
    "\n",
    "In this notebook, you'll learn how collaborative filtering works and see examples of its application. You'll also get to know (or remeber!) SciPy's sparse matrixes, which can be very handy with big datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two classical recommendation problems that we can solve with collaborative filtering\n",
    "\n",
    "### Best item problem\n",
    "\n",
    "In the best item problem, we're trying to recommend to a user, the new item that he will be most interested in.\n",
    "\n",
    "For instance, in the context of movie recommendations, where users rate movies (explicit feedback), this problem consists in predicting the rating that a user would give to each unseen movie and then suggesting him the movie for which we predicted the highest rating.\n",
    "\n",
    "Evaluation metrics that can be used in this problem are:\n",
    "* [Mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "* [Root mean square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
    "\n",
    "\n",
    "### Top N items problem\n",
    "\n",
    "In this problem, we're trying to find the list of N items that the user will have most interest to interact with.\n",
    "\n",
    "For instance, in online marketplaces, like Amazon, it's common to study the users' interactions with the products in terms of clicks (implicit feedback). So, in this case, the top N items problem consists in selecting the list of items that the user will most likely click in.\n",
    "\n",
    "Evaluation metrics that can be used in this problem are:\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) (how many items in the top N did the user click in)\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) (how many items that had clicks were in the top N)\n",
    "* [Mean average precision](https://en.wikipedia.org/wiki/Information_retrieval#Mean_average_precision)  (same idea as precision, but accounting for the order of the items in the top N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition on collaborative filtering methods\n",
    "\n",
    "\n",
    "In a very high level way, and using the movies scenario, there are two ideas behind collaborative filtering:\n",
    "- If two people usually like the same movies and dislike the same movies, then they will probably feel the same way about a new movie\n",
    "- If many people feel like two movies, then the two movies are probably similar; a user that liked one of the movies will probably like the other movie too\n",
    "\n",
    "Now, let's dig into each of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User based method\n",
    "\n",
    "The first idea presented above corresponds to the user based method of collaborative filtering.\n",
    "\n",
    "In this method, when we're trying to recommend an item to a user _u_, we start by finding _u_'s nearest neighbors among the other users, which are the users that interact with items in the same way as _u_ does.\n",
    "Then, we use the nearest neighbors' ratings to predict u's ratings for unseen movies. Finally, the recommended movie will be the one with the highest predicted rating.\n",
    "\n",
    "For example, considering this ratings table, we are trying to recommend a movie for Alice to watch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MOVIES_FILE, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both Alice and Bob seem to like science fiction movies and dislike musicals. With Cate it's the oposite, and Dylan has an eclectic taste.\n",
    "So, we can say that for Alice, Bob is her most similar neighbor, followed by Dylan. And Cate is the least similar one.\n",
    "\n",
    "##### Similarity measures\n",
    "\n",
    "But how do we measure the actual similarities between them?\n",
    "Many possible solutions:\n",
    "* [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "* [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "* [Spearman rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "\n",
    "Let's use pearson correlation to compute the similarity between each pair of users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataframe again in order to treat the - as Nan\n",
    "df = pd.read_csv(MOVIES_FILE, index_col=0, na_values='-')\n",
    "S = df.corr(method='pearson', min_periods=1)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's scale the similarity values from 0 to 1 (this is to guarantee that our recommendation will have the same scale as the user ratings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_correlation_matrix(S):\n",
    "    S_min = S.min().min()\n",
    "    S_max = S.max().max()\n",
    "    return (S - S_min) / (S_max - S_min)\n",
    "\n",
    "\n",
    "S = scale_correlation_matrix(S)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict Alice's ratings for the unseen movies \"Singing in the rain\" and \"Interstellar\".\n",
    "\n",
    "For this, we'll use the following equation, which basically says that the predicted rating for an item is the average of the nearest neighbors' ratings, weighted by the similarities between the user and the neighbors:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "r_{ui}' = \\frac{\\sum_{v \\in N_i(u)} w_{uv} r_{vi}}{\\sum_{v \\in N_i(u)} |w_{uv}|}\n",
    "\\end{equation*}\n",
    "\n",
    "$r_{ui}'$: _predicted rating from user u to item i_\n",
    "\n",
    "$N_i(u)$: _nearest neighbors of user u that have rated item i_\n",
    "\n",
    "$w_{uv}$: _similarity between users u and v_\n",
    "\n",
    "$r_{vi}$: _rating given by user v to item i_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_similarities = S.loc['Alice'].drop('Alice')\n",
    "\n",
    "singing_in_the_rain = df.loc['Singing in the rain'].drop('Alice')\n",
    "r_singing_in_the_rain = np.average(singing_in_the_rain, weights=alice_similarities)\n",
    "\n",
    "interstellar = df.loc['Interstellar'].drop('Alice')\n",
    "r_interstellar = np.average(interstellar, weights=alice_similarities)\n",
    "\n",
    "print('* \"Singing in the rain\" predicted rating: {}'.format(round(r_singing_in_the_rain, 2)))\n",
    "print('* \"Interstellar\"        predicted rating: {}'.format(round(r_interstellar, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best recommendation we can give to Alice is for her to watch \"Interstellar\" (and not to watch \"Singing in the rain\", since she'll probably hate it!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"giphy-downsized.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item based method\n",
    "\n",
    "The item based approach follows the second idea that we presented above.\n",
    "\n",
    "In this case, we will use similarities between items, instead of between users.\n",
    "And to predict the rating that user _u_ gave to item _i_, we'll use the average of the ratings that user _u_ gave to the items most similar to _i_, weighted by the similarities between items.\n",
    "\n",
    "The equation for the model is, in this case:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "r_{ui}' = \\frac{\\sum_{j \\in N_u(i)} w_{ij} r_{ui}}{\\sum_{j \\in N_u(i)} |w_{ij}|}\n",
    "\\end{equation*}\n",
    "\n",
    "$r_{ui}'$: _predicted rating from user u to item i_\n",
    "\n",
    "$N_u(i)$: _nearest neighbors of item i that user u has rated_\n",
    "\n",
    "$w_{ij}$: _similarity between items i and j_\n",
    "\n",
    "$r_{ui}$: _rating given by user u to item i_\n",
    "\n",
    "\n",
    "Let's see this case with our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're transposing the dataframe in order to compute correlations between the columns (movies)\n",
    "df_items = df.transpose()\n",
    "S = df_items.corr(method='pearson', min_periods=1).fillna(0) # can i do this?\n",
    "S = scale_correlation_matrix(S)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_ratings = df[['Alice']].dropna()\n",
    "alice_movies = alice_ratings.index\n",
    "alice_ratings = alice_ratings.Alice.values\n",
    "\n",
    "singing_in_the_rain_similarities = S.loc[alice_movies, 'Singing in the rain'].values\n",
    "r_singing_in_the_rain = np.average(alice_ratings, weights=singing_in_the_rain_similarities)\n",
    "\n",
    "interstellar_similarities = S.loc[alice_movies, 'Interstellar'].values\n",
    "r_interstellar = np.average(alice_ratings, weights=interstellar_similarities)\n",
    "\n",
    "print('* \"Singing in the rain\" predicted rating: {}'.format(round(r_singing_in_the_rain, 2)))\n",
    "print('* \"Interstellar\"        predicted rating: {}'.format(round(r_interstellar, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method, we are not so reluctant in recommending \"Singing in the rain\" to Alice!\n",
    "When we compute the correlation between items, based on user actions, we get a result that says that \"Interstellar\" and \"Avatar\" are not similar at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User based vs Item based\n",
    "\n",
    "When should each method be used?\n",
    "\n",
    "* If our system has much __more users than items, go for item based__ (and vice-versa). Computing similarities is probably the most expensive operation you'll be doing, thus do it in the smaller set.\n",
    "\n",
    "* If your items are more stable than users, i.e, if you __add users more frequently than items, go for item based__ (and vice-versa). You'll want to recompute the similarities matrix as least as possible (while keeping accurate results, of course!\n",
    "\n",
    "* Do you need [__serendipity__](https://en.wikipedia.org/wiki/Serendipity)? If yes, go for __user based__. When you use item based, given that the results are based on the user's actions, the results will be less surprising.\n",
    "\n",
    "* Do you need to __justify your recommendations__? If yes, go for __item based__. It's easier to justify a recommendation with: \"We're showing you this because you liked that\" than with: \"We're showing you this because Jon Doe (who you don't know!) also liked it and your tastes are very similar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now imagine that ended up recommending both movies to Alice and after she watched them, she gave us her rating:\n",
    "* Singing in the rain => 3\n",
    "* Interstellar => 5\n",
    "\n",
    "With the true results, we can compute the RMSE to evaluate our recommendations, for both the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [3, 5]\n",
    "\n",
    "# User based\n",
    "y_pred = [2.33, 5]\n",
    "print('* User based RMSE = {}'.format(round(sqrt(mean_squared_error(y_true, y_pred)), 4)))\n",
    "\n",
    "# Item based\n",
    "y_pred = [3, 3.43]\n",
    "print('* Item based RMSE = {}'.format(round(sqrt(mean_squared_error(y_true, y_pred)), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we want to give Top N recommendations and we have implicit feedback?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(to be completed)\n",
    "1. implicit feedback: more interactions isn't necessarily better\n",
    "2. implicit feedback: more noise\n",
    "3. top N: basically the same approach (similarity * actions), but this time, we can recommend more items and the order matters!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPy sparse matrices\n",
    "\n",
    "A sparse matrix is a big matrix where most of its elements are 0.\n",
    "Handling very big data structures can be a pain in our laptops where we have limited memory and processing power.\n",
    "So, the smart people who build math libraries (like [SciPy](https://docs.scipy.org/doc/scipy/reference/sparse.html)), developed the sparce matrices data structures.\n",
    "\n",
    "Basically the idea behind sparce matrices is to just store the information relative to the non-zero elements of the matrix.\n",
    "This makes working with these matrices way easier and faster than if we used regular Numpy matrices or Pandas.\n",
    "\n",
    "We'll start this section with some concepts regarding sparce matrices and then we'll see a practical example, which is to compute cosine similarities between items using a sparse matrix.\n",
    "\n",
    "There are several types of sparse matrices, some of which are:\n",
    "\n",
    "##### [coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix)\n",
    "Matrix in the coordinates format.\n",
    "\n",
    "__+__ it's a fast format for constructing sparse matrices\n",
    "\n",
    "__-__ doesn't directly support arithmetic operations or slicing, so requires conversion to other formats\n",
    "\n",
    "##### [csc_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix)\n",
    "Compressed sparse column matrix.\n",
    "\n",
    "__+__ efficient arithmetic operations (CSC + CSC, CSC * CSC, etc.)\n",
    "\n",
    "__+__ efficient column slicing\n",
    "\n",
    "__-__ slow row slicing operations (consider CSR)\n",
    "\n",
    "__-__ changes to the sparsity structure are expensive (consider LIL or DOK)\n",
    "\n",
    "##### [csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)\n",
    "Compressed sparse row matrix.\n",
    "\n",
    "__+__ efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
    "\n",
    "__+__ efficient row slicing\n",
    "\n",
    "__+__ fast matrix vector products\n",
    "\n",
    "__-__ slow column slicing operations (consider CSC)\n",
    "\n",
    "__-__ changes to the sparsity structure are expensive (consider LIL or DOK)\n",
    "\n",
    "##### [lil_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix)\n",
    "Row-based linked list sparse matrix.\n",
    "\n",
    "__+__ supports flexible slicing\n",
    "\n",
    "__+__ changes to the matrix sparsity structure are efficient\n",
    "\n",
    "__-__ arithmetic operations LIL + LIL are slow (consider CSR or CSC)\n",
    "\n",
    "__-__ slow column slicing (consider CSC)\n",
    "\n",
    "__-__ slow matrix vector products (consider CSR or CSC)\n",
    "\n",
    "\n",
    "For this example we'll use the Movielens-100k data set, which is a list of records with users, items and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sparse matrix in the coordinates format (coo_matrix), where rows are items and columns are users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row  = np.array(df.item_id.values)\n",
    "col  = np.array(df.user_id.values)\n",
    "data = np.array(df.rating.values)\n",
    "m = coo_matrix((data, (row, col)), shape=(df.item_id.max()+1, df.user_id.max()+1))\n",
    "m.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's compute the density of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of non-zero values\n",
    "nnz = m.nnz\n",
    "\n",
    "# get the matrix shape\n",
    "shape = m.shape\n",
    "\n",
    "# the number of elements in the matrix is n_rows * n_cols\n",
    "n_elems = shape[0] * shape[1]\n",
    "\n",
    "# the density of the matrix is the number of non-zero elements over all the elements\n",
    "print('Density of sparse matrix: {}%'.format(round(nnz/n_elems*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the cosine similarity between items of the matrix, we'll use scikit's [cosine_similarity](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html). This method can return the result in form of a sparse matrix (csr_matrix) by setting the dense_output flag to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = cosine_similarity(m, dense_output=False)\n",
    "print(type(S))\n",
    "S.toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
